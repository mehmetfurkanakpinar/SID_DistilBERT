{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from transformers import  AutoModelForSequenceClassification, AutoTokenizer\n",
    "# Load the pretrained DistilBERT model and tokenizer\n",
    "checkpoint = \"distilbert-base-cased\" \n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "file_path = \"path-to-train_dataset\"\n",
    "df = pd.read_excel(file_path, engine='openpyxl')"
   ],
   "id": "39e00c1f87ee01ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"class\"] = label_encoder.fit_transform(df[\"class\"])"
   ],
   "id": "d30a5dc5d53a138e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split dataset as training and evaluation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_eval = train_test_split(df, train_size=0.8,stratify=df[\"class\"], random_state=42)"
   ],
   "id": "f14f72d7c26b8dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Hugging Face datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"eval\": Dataset.from_pandas(df_eval)\n",
    "})"
   ],
   "id": "d54e095ba8aa4946",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Dataset Dict:\\n\", raw_datasets)\n",
    "print(\"\\n\\nTrain's features:\\n\", raw_datasets[\"train\"].features)\n",
    "print(\"\\n\\nFirst row of Train:\\n\", raw_datasets[\"train\"][0])"
   ],
   "id": "cde0347b860eb64f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make sure the text is string and tokenize the datasets\n",
    "raw_datasets = raw_datasets.map(lambda dataset: {'text': str(dataset['text'])}, batched=False)\n",
    "tokenized_datasets = raw_datasets.map(lambda dataset: tokenizer(dataset['text'], truncation=True), batched=True)"
   ],
   "id": "49e28bfe4c9fae3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_datasets)",
   "id": "2fdd034b422d01b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_datasets[\"train\"][0])",
   "id": "793c0350d7eaa155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove unnecessary columns if there is any and rename the class column as labels\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"class\", \"labels\")\n",
    "print(tokenized_datasets)"
   ],
   "id": "d36979dcf592ba92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Disable parallelism for tokenizers\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "2c2cd1b9c3790605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_dir = os.path.join(\"/data1/ma2\", \"checkpoints\")\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    print(f\"Checkpoint file is created: {checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"Checkpoint file already exists: {checkpoint_dir}\")"
   ],
   "id": "5ea8482cd1bd0466",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Set up training arguments and trainer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training args \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_dir,              \n",
    "    num_train_epochs=5,                    \n",
    "    eval_strategy=\"epoch\",            \n",
    "    weight_decay=5e-4,                    \n",
    "    save_strategy=\"epoch\",         \n",
    "    save_total_limit=3,                \n",
    "    report_to=\"none\",                      \n",
    "    load_best_model_at_end=True,         \n",
    "    metric_for_best_model=\"accuracy\"   \n",
    ")\n",
    "\n",
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\") # F1 and Accuracy\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    classifier,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_loss=lambda model, inputs: loss_fn(model(inputs).logits, inputs['labels']),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] \n",
    ")"
   ],
   "id": "f0684a0481e1e4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "trainer.train()"
   ],
   "id": "8b2fd8cc8078100e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Veriyi yükleyin\n",
    "X = df['text']\n",
    "y = df['class']\n",
    "\n",
    "# Cross-validation ayarları\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "eval_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Tokenize the data\n",
    "    train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(X_val), truncation=True, padding=True)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': y_train})\n",
    "    val_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask'], 'labels': y_val})\n",
    "\n",
    "    # Trainer and training\n",
    "    trainer = Trainer(\n",
    "        model=classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Record the results\n",
    "    train_losses.append(trainer.state.log_history[-1]['loss'])\n",
    "    eval_result = trainer.evaluate()\n",
    "    eval_losses.append(eval_result['eval_loss'])\n",
    "    eval_accuracies.append(eval_result['eval_accuracy'])\n",
    "\n",
    "# Print the Cross-validation results\n",
    "print(f\"Cross-validation Train Loss: {np.mean(train_losses)}\")\n",
    "print(f\"Cross-validation Eval Loss: {np.mean(eval_losses)}\")\n",
    "print(f\"Cross-validation Eval Accuracy: {np.mean(eval_accuracies)}\")"
   ],
   "id": "18ed38c0d2d38075",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
